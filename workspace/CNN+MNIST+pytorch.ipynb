{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caf02768",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DuncanJF/dsml_jupyter/blob/main/workspace/CNN%2BMNIST%2Bpytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c1d63-6530-4443-9a14-6cc843ffa05a",
   "metadata": {
    "id": "639c1d63-6530-4443-9a14-6cc843ffa05a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e929f-c8c6-4b00-af22-0be924d7b636",
   "metadata": {
    "id": "bb3e929f-c8c6-4b00-af22-0be924d7b636"
   },
   "outputs": [],
   "source": [
    "#STEP 1: LOADING DATASET\n",
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                            train=True,\n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeef554-5df2-40f8-b833-1381eebd1a0d",
   "metadata": {
    "id": "3eeef554-5df2-40f8-b833-1381eebd1a0d",
    "outputId": "c2fcfe22-9916-448b-c5ff-4200f1f2388c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17830a-2ed8-4eaf-b4cc-2a2f58ddb4c4",
   "metadata": {
    "id": "cc17830a-2ed8-4eaf-b4cc-2a2f58ddb4c4"
   },
   "outputs": [],
   "source": [
    "test_dataset = dsets.MNIST(root='./data',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40cb41f-febe-4551-ac15-81b6fd994b21",
   "metadata": {
    "id": "a40cb41f-febe-4551-ac15-81b6fd994b21",
    "outputId": "be2cc161-a018-43ec-ff53-1168b66d0e17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e110f5-0f8a-47fb-a4a7-b532ee535937",
   "metadata": {
    "id": "68e110f5-0f8a-47fb-a4a7-b532ee535937"
   },
   "outputs": [],
   "source": [
    "#STEP 2: MAKING DATASET ITERABLE\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc80b25b-ac2c-489d-b3f8-0dfde89878e6",
   "metadata": {
    "id": "fc80b25b-ac2c-489d-b3f8-0dfde89878e6"
   },
   "outputs": [],
   "source": [
    "#STEP 3: CREATE MODEL CLASS\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function 1: 784 --> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # Non-linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Linear function 2: 100 --> 100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Linear function 3: 100 --> 100\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 3\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        # Linear function 4 (readout): 100 --> 10\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function 1\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        # Linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        # Linear function 2\n",
    "        out = self.fc3(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu3(out)\n",
    "\n",
    "        # Linear function 4 (readout)\n",
    "        out = self.fc4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342dc065-9284-4654-b451-c236d87f61d2",
   "metadata": {
    "id": "342dc065-9284-4654-b451-c236d87f61d2"
   },
   "outputs": [],
   "source": [
    "#STEP 4: INSTANTIATE MODEL CLASS\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794fcb4-75ae-43d0-9e7c-1c270d099a85",
   "metadata": {
    "id": "d794fcb4-75ae-43d0-9e7c-1c270d099a85"
   },
   "outputs": [],
   "source": [
    "#STEP 5: INSTANTIATE LOSS CLASS\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b7a66-4c33-49c7-a327-41ea961d21d3",
   "metadata": {
    "id": "c36b7a66-4c33-49c7-a327-41ea961d21d3"
   },
   "outputs": [],
   "source": [
    "#STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ab9c9a-3eff-40f2-bc56-baf9f6572530",
   "metadata": {
    "id": "23ab9c9a-3eff-40f2-bc56-baf9f6572530",
    "outputId": "2177315e-4ba8-46de-ba06-8602cdbed418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e7f22-d28d-4836-955e-35478bb849eb",
   "metadata": {
    "id": "b54e7f22-d28d-4836-955e-35478bb849eb",
    "outputId": "7af8b54e-5366-4404-c66f-1eaad6cee716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1. Loss: 2.301947593688965. Accuracy: 9.739999771118164\n",
      "Iteration: 2. Loss: 2.297227382659912. Accuracy: 9.739999771118164\n",
      "Iteration: 4. Loss: 2.3088860511779785. Accuracy: 9.739999771118164\n",
      "Iteration: 8. Loss: 2.297534942626953. Accuracy: 9.789999961853027\n",
      "Iteration: 16. Loss: 2.294639825820923. Accuracy: 18.799999237060547\n",
      "Iteration: 32. Loss: 2.274282455444336. Accuracy: 26.209999084472656\n",
      "Iteration: 64. Loss: 2.232170343399048. Accuracy: 41.400001525878906\n",
      "Iteration: 128. Loss: 1.244429588317871. Accuracy: 58.599998474121094\n",
      "Iteration: 256. Loss: 0.5970173478126526. Accuracy: 84.77999877929688\n",
      "Iteration: 512. Loss: 0.3231336176395416. Accuracy: 90.33999633789062\n",
      "Iteration: 1024. Loss: 0.09996500611305237. Accuracy: 93.7699966430664\n",
      "Iteration: 2048. Loss: 0.06200489029288292. Accuracy: 96.1500015258789\n"
     ]
    }
   ],
   "source": [
    "#STEP 7: TRAIN THE MODEL\n",
    "iter = 0\n",
    "iteration_prints=[1,2,4,8,16,32,64,128,256,512,1024,2048,4096,8192]\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        #if iter % 500 == 0:\n",
    "        if iter in iteration_prints:\n",
    "            # Calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9fac26-ce6a-4a46-a312-efb18d44ae9f",
   "metadata": {
    "id": "2c9fac26-ce6a-4a46-a312-efb18d44ae9f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
